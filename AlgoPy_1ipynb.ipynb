{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRX5IceEvYKicD3AVn4BTm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aagamaar/Algo_Trading_Python/blob/main/AlgoPy_1ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MOUNTED GOOGLE DRIVE"
      ],
      "metadata": {
        "id": "gH8Bo1BQO6QV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nda8qjnqNoH4",
        "outputId": "8bc50fbd-a39b-4d8b-d329-88b6775e2ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully!\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATING PROJECT DIRECTORY STRUCTURE"
      ],
      "metadata": {
        "id": "AKF-zls0PBTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Project Directory Structure\n",
        "import os\n",
        "\n",
        "# Defining the project root in Google Drive\n",
        "project_root = '/content/drive/MyDrive/algo_trading_prototype'\n",
        "\n",
        "# Creating the main project directory\n",
        "os.makedirs(project_root, exist_ok=True)\n",
        "print(f\"Project root created: {project_root}\")\n",
        "\n",
        "# Creating subdirectories\n",
        "subdirectories = [\n",
        "    'config',\n",
        "    'data',\n",
        "    'strategy',\n",
        "    'backtester',\n",
        "    'analytics',\n",
        "    'sheets',\n",
        "    'utils',\n",
        "    'logs' # Directory for log files\n",
        "]\n",
        "\n",
        "for subdir in subdirectories:\n",
        "    path = os.path.join(project_root, subdir)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    print(f\"Created directory: {path}\")\n",
        "\n",
        "# I have added __init__.py files to make directories into Python packages\n",
        "# This is crucial for Python to recognize the folders as modules for importing.\n",
        "\n",
        "for pkg_dir in ['data', 'strategy', 'backtester', 'analytics', 'sheets', 'utils']:\n",
        "    with open(os.path.join(project_root, pkg_dir, '__init__.py'), 'w') as f:\n",
        "        pass # Created an empty __init__.py file\n",
        "    print(f\"Created __init__.py in {pkg_dir}\")\n",
        "\n",
        "print(\"\\nDirectory structure created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1SjuFktOJoK",
        "outputId": "41776b0c-e8aa-46e8-c0c5-cbec10c22ade"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project root created: /content/drive/MyDrive/algo_trading_prototype\n",
            "Created directory: /content/drive/MyDrive/algo_trading_prototype/config\n",
            "Created directory: /content/drive/MyDrive/algo_trading_prototype/data\n",
            "Created directory: /content/drive/MyDrive/algo_trading_prototype/strategy\n",
            "Created directory: /content/drive/MyDrive/algo_trading_prototype/backtester\n",
            "Created directory: /content/drive/MyDrive/algo_trading_prototype/analytics\n",
            "Created directory: /content/drive/MyDrive/algo_trading_prototype/sheets\n",
            "Created directory: /content/drive/MyDrive/algo_trading_prototype/utils\n",
            "Created directory: /content/drive/MyDrive/algo_trading_prototype/logs\n",
            "Created __init__.py in data\n",
            "Created __init__.py in strategy\n",
            "Created __init__.py in backtester\n",
            "Created __init__.py in analytics\n",
            "Created __init__.py in sheets\n",
            "Created __init__.py in utils\n",
            "\n",
            "Directory structure created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSTALLING THE DEPENDENCIES"
      ],
      "metadata": {
        "id": "7MvCiRG1QMSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the Dependencies\n",
        "# Used --quiet to suppress verbose output\n",
        "!pip uninstall -y gspread google-auth google-auth-oauthlib google-api-python-client\n",
        "!pip install --no-cache-dir gspread google-auth google-auth-oauthlib google-api-python-client\n",
        "!pip install pandas numpy ta gspread oauth2client scikit-learn requests yfinance python-telegram-bot --quiet\n",
        "print(\"All required Python dependencies installed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yzg7TpdmPLt5",
        "outputId": "1dc0d521-0ba4-4dcb-d65d-643766400e74"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gspread 6.2.1\n",
            "Uninstalling gspread-6.2.1:\n",
            "  Successfully uninstalled gspread-6.2.1\n",
            "Found existing installation: google-auth 2.40.3\n",
            "Uninstalling google-auth-2.40.3:\n",
            "  Successfully uninstalled google-auth-2.40.3\n",
            "Found existing installation: google-auth-oauthlib 1.2.2\n",
            "Uninstalling google-auth-oauthlib-1.2.2:\n",
            "  Successfully uninstalled google-auth-oauthlib-1.2.2\n",
            "Found existing installation: google-api-python-client 2.174.0\n",
            "Uninstalling google-api-python-client-2.174.0:\n",
            "  Successfully uninstalled google-api-python-client-2.174.0\n",
            "Collecting gspread\n",
            "  Downloading gspread-6.2.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting google-auth\n",
            "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting google-auth-oauthlib\n",
            "  Downloading google_auth_oauthlib-1.2.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting google-api-python-client\n",
            "  Downloading google_api_python_client-2.174.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.25.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.6.15)\n",
            "Downloading gspread-6.2.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m134.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.1/216.1 kB\u001b[0m \u001b[31m251.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.2.2-py3-none-any.whl (19 kB)\n",
            "Downloading google_api_python_client-2.174.0-py3-none-any.whl (13.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-auth, google-auth-oauthlib, gspread, google-api-python-client\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-api-python-client-2.174.0 google-auth-2.40.3 google-auth-oauthlib-1.2.2 gspread-6.2.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "7c9095ce5add4ce0ae608f5d7fa57c00"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All required Python dependencies installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HOLDS ALL CONFIGURATIONS"
      ],
      "metadata": {
        "id": "64ow2GkpQ75m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: config/settings.py\n",
        "%%writefile /content/drive/MyDrive/algo_trading_prototype/config/settings.py\n",
        "# config/settings.py\n",
        "\n",
        "# Stock Data API (using yfinance)\n",
        "# No API key is needed for basic yfinance usage\n",
        "STOCK_SYMBOLS = [\"RELIANCE.NS\", \"TCS.NS\", \"HDFCBANK.NS\"] # Three NIFTY 50 stocks for NSE.\n",
        "\n",
        "# Google Sheets Configuration\n",
        "GOOGLE_SHEET_ID = '1W8nP1H7oIKwdy7m5dGii0lZcf_cKxn0mPSdzhAddm80' # Replace with your actual Google Sheet ID\n",
        "GOOGLE_SHEET_TRADELOG_WORKSHEET_NAME = 'TradeLog'\n",
        "GOOGLE_SHEET_PNL_WORKSHEET_NAME = 'Summary PnL'\n",
        "TELEGRAM_ALERTS_WORKSHEET_NAME = 'Alerts'\n",
        "\n",
        "# Telegram Configuration\n",
        "TELEGRAM_BOT_TOKEN = 'YOUR_BOT_TOKEN' # Replace with your Telegram Bot Token\n",
        "TELEGRAM_CHAT_ID = 'YOUR_CHAT_ID'     # Replace with your Telegram Chat ID\n",
        "\n",
        "# Google Sheets\n",
        "GOOGLE_SHEET_ID = \"1W8nP1H7oIKwdy7m5dGii0lZcf_cKxn0mPSdzhAddm80\"\n",
        "TRADE_LOG_SHEET_NAME = \"Trade Log\"\n",
        "SUMMARY_PL_SHEET_NAME = \"Summary P&L\"\n",
        "WIN_RATIO_SHEET_NAME = \"Win Ratio\"\n",
        "\n",
        "# Strategy Parameters\n",
        "RSI_PERIOD = 14\n",
        "RSI_BUY_THRESHOLD = 30\n",
        "SHORT_MA_PERIOD = 20 # 20-Day Moving Average\n",
        "LONG_MA_PERIOD = 50  # 50-Day Moving Average\n",
        "\n",
        "# Backtesting Parameters\n",
        "BACKTEST_DURATION_MONTHS = 6\n",
        "\n",
        "# ML Model Parameters\n",
        "FEATURES = ['RSI', 'MACD', 'Volume', 'Close'] # Features for ML model\n",
        "TARGET = 'Next_Day_Movement' # Targets for ML model\n",
        "\n",
        "# Telegram Alerts (Bonus)\n",
        "TELEGRAM_BOT_TOKEN = \"8144019769:AAF-f7tW-XV9URIgJAAFyQgNtE0Tce0naXw\"\n",
        "TELEGRAM_CHAT_ID = \"1463467106\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mU0VgdkQTtP",
        "outputId": "ee29fd51-6ddb-474d-9c98-65ed7137d0e8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/algo_trading_prototype/config/settings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SETTING UP LOGGING TO BOTH CONSOLE AND A FILE IN LOGS DIRECTORY"
      ],
      "metadata": {
        "id": "LnqI9HmOR4Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/algo_trading_prototype/utils/logger.py\n",
        "# utils/logger.py\n",
        "\n",
        "import logging\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def setup_logging():\n",
        "    \"\"\"Configures logging for the application.\"\"\"\n",
        "    # Adjusted log_dir to be within your mounted Google Drive project structure.\n",
        "    log_dir = '/content/drive/MyDrive/algo_trading_prototype/logs'\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    log_file = os.path.join(log_dir, f\"algo_trading_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO, # Log INFO, WARNING, ERROR, CRITICAL messages\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler(log_file), # Log to a file\n",
        "            logging.StreamHandler()        # Also print to console\n",
        "        ]\n",
        "    )\n",
        "    # Suppressing verbose logging from libraries to keep console clean\n",
        "    logging.getLogger('requests').setLevel(logging.WARNING)\n",
        "    logging.getLogger('urllib3').setLevel(logging.WARNING)\n",
        "    logging.getLogger('yfinance').setLevel(logging.WARNING)\n",
        "    logging.getLogger('gspread').setLevel(logging.WARNING)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0sF6e_GR3lY",
        "outputId": "8ee514cf-cdc4-4350-e50d-75b73a875d94"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/algo_trading_prototype/utils/logger.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HANDLES FETCHING STOCK DATA USING YFINANCE"
      ],
      "metadata": {
        "id": "4_Vh_wcvSvW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/algo_trading_prototype/data/data_fetcher.py\n",
        "# data/data_fetcher.py\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "from config import settings\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class DataFetcher:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fetch_historical_data(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:\n",
        "        try:\n",
        "            df = yf.download(symbol, start=start_date, end=end_date, progress=False)\n",
        "            if df.empty:\n",
        "                logger.warning(f\"No data fetched for {symbol} between {start_date} and {end_date}.\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            # Ensure index is a clean DatetimeIndex and named 'Date'\n",
        "            df.index = pd.to_datetime(df.index)\n",
        "            df.index.name = 'Date'\n",
        "\n",
        "            # --- CRITICAL FIX FOR YFINANCE COLUMNS (Revised) ---\n",
        "            # Step 1: Flatten MultiIndex columns if they exist.\n",
        "            # This is common, and the format is usually ('ColumnName', 'SymbolSuffix') or ('ColumnName', '')\n",
        "            if isinstance(df.columns, pd.MultiIndex):\n",
        "                # We want just 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close'\n",
        "                # Let's extract the first level of the MultiIndex or the full string if it's not a tuple\n",
        "                new_columns = []\n",
        "                for col in df.columns:\n",
        "                    if isinstance(col, tuple):\n",
        "                        # Take the main part (e.g., 'Close' from ('Close', 'RELIANCE.NS'))\n",
        "                        new_columns.append(col[0].strip())\n",
        "                    else:\n",
        "                        new_columns.append(col.strip())\n",
        "                df.columns = new_columns\n",
        "                logger.info(f\"Flattened MultiIndex columns for {symbol}. Raw flattened columns: {df.columns.tolist()}\")\n",
        "\n",
        "            # Step 2: Standardize column names and handle 'Adj Close'\n",
        "            # Convert all column names to standard format (e.g., 'Open', 'Close')\n",
        "            # Use .str.capitalize() on a Series of column names to handle variations like 'open' -> 'Open'\n",
        "            df.columns = df.columns.str.replace(' ', '').str.capitalize() # Remove spaces, then capitalize (e.g., \"Adj Close\" -> \"AdjClose\" -> \"Adjclose\")\n",
        "\n",
        "            # Check for 'Adj Close' (which would now be 'Adjclose' or 'AdjClose')\n",
        "            if 'Adjclose' in df.columns:\n",
        "                df['Close'] = df['Adjclose'] # Use Adjusted Close as the primary 'Close'\n",
        "                df.drop(columns=['Adjclose'], inplace=True)\n",
        "                logger.info(f\"Used 'Adjclose' as 'Close' for {symbol}.\")\n",
        "            elif 'Close' not in df.columns:\n",
        "                 logger.error(f\"Neither 'Close' nor 'Adjclose' found for {symbol}. Available: {df.columns.tolist()}\")\n",
        "                 return pd.DataFrame()\n",
        "\n",
        "\n",
        "            # Ensure only the desired columns are present and in order\n",
        "            required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "            if not all(col in df.columns for col in required_columns):\n",
        "                missing = [col for col in required_columns if col not in df.columns]\n",
        "                logger.error(f\"Missing one or more critical columns ({missing}) after standardization for {symbol}. Available: {df.columns.tolist()}\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            df = df[required_columns] # Select the columns in the desired order\n",
        "\n",
        "            logger.info(f\"Successfully fetched and prepared historical data for {symbol}. Final columns: {df.columns.tolist()}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching data for {symbol} using yfinance: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "# This function must have ZERO indentation\n",
        "def get_historical_data(symbols: list, duration_months: int) -> dict:\n",
        "    \"\"\"\n",
        "    Fetches historical data for multiple symbols for the last 'duration_months' using yfinance.\n",
        "    Parameters:\n",
        "        symbols (list): List of stock ticker symbols.\n",
        "        duration_months (int): Number of months for which to fetch historical data.\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are symbols and values are pandas DataFrames.\n",
        "    \"\"\"\n",
        "    data_fetcher = DataFetcher()\n",
        "    all_data = {}\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(days=duration_months * 30)\n",
        "\n",
        "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
        "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
        "\n",
        "    for symbol in symbols:\n",
        "        df = data_fetcher.fetch_historical_data(symbol, start_date_str, end_date_str)\n",
        "        if not df.empty:\n",
        "            all_data[symbol] = df\n",
        "    return all_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0clOdM0IQ6lD",
        "outputId": "1eb660ec-ab01-499b-853b-e10dd2f05607"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/algo_trading_prototype/data/data_fetcher.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FUNCTIONS TO CALCULATE TECHNICAL INDICATORS LIKE RSI AND MOVING AVERAGES"
      ],
      "metadata": {
        "id": "dLoj7u1STRdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: strategy/indicators.py\n",
        "%%writefile /content/drive/MyDrive/algo_trading_prototype/strategy/indicators.py\n",
        "# strategy/indicators.py\n",
        "\n",
        "import pandas as pd\n",
        "import ta # Technical Analysis library\n",
        "\n",
        "def calculate_rsi(df: pd.DataFrame, window: int = 14) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Calculates Relative Strength Index (RSI).\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): DataFrame with 'Close' prices.\n",
        "        window (int): The window period for RSI calculation.\n",
        "    Returns:\n",
        "        pd.Series: A Series containing RSI values.\n",
        "    \"\"\"\n",
        "    return ta.momentum.RSIIndicator(df[\"Close\"].squeeze(), window=window).rsi()\n",
        "\n",
        "def calculate_sma(df: pd.DataFrame, window: int) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Calculates Simple Moving Average (SMA).\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): DataFrame with 'Close' prices.\n",
        "        window (int): The window period for SMA calculation.\n",
        "    Returns:\n",
        "        pd.Series: A Series containing SMA values.\n",
        "    \"\"\"\n",
        "    return ta.trend.SMAIndicator(df[\"Close\"], window=window).sma_indicator()\n",
        "\n",
        "def calculate_macd(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculates Moving Average Convergence Divergence (MACD), Signal Line, and MACD Histogram.\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): DataFrame with 'Close' prices.\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with 'MACD', 'MACD_Signal', and 'MACD_Hist' columns.\n",
        "    \"\"\"\n",
        "    # Fixed: Ensure df[\"Close\"] is explicitly a Series using .squeeze()\n",
        "    macd_indicator = ta.trend.MACD(df[\"Close\"].squeeze()) # <<< ADD .squeeze() here\n",
        "    return pd.DataFrame({\n",
        "        'MACD': macd_indicator.macd(),\n",
        "        'MACD_Signal': macd_indicator.macd_signal(),\n",
        "        'MACD_Hist': macd_indicator.macd_diff()\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPNezTKCSpm7",
        "outputId": "f7009ec8-22e2-4609-b9a3-736eb3dac62a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/algo_trading_prototype/strategy/indicators.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPLEMENTS THE TRADING STRATEGY LOGIC (RSI + Moving Average crossover)"
      ],
      "metadata": {
        "id": "FzRkLo0RTjO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: strategy/trading_strategy.py\n",
        "%%writefile /content/drive/MyDrive/algo_trading_prototype/strategy/trading_strategy.py\n",
        "# strategy/trading_strategy.py\n",
        "\n",
        "import pandas as pd\n",
        "import logging\n",
        "from strategy.indicators import calculate_rsi, calculate_sma\n",
        "from config import settings # Import settings module directly\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class TradingStrategy:\n",
        "    def __init__(self):\n",
        "        self.rsi_period = settings.RSI_PERIOD\n",
        "        self.rsi_buy_threshold = settings.RSI_BUY_THRESHOLD\n",
        "        self.short_ma_period = settings.SHORT_MA_PERIOD\n",
        "        self.long_ma_period = settings.LONG_MA_PERIOD\n",
        "\n",
        "    def generate_signals(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Generates buy/sell signals based on RSI and Moving Average Crossover strategy.\n",
        "        Adds 'RSI', 'SMA_short', 'SMA_long', 'Signal' (1=Buy, -1=Sell, 0=Hold), and 'Position' columns.\n",
        "        Parameters:\n",
        "            df (pd.DataFrame): DataFrame with historical OHLCV data.\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with added indicator, signal, and position columns.\n",
        "        \"\"\"\n",
        "        if df.empty:\n",
        "            logger.warning(\"Empty DataFrame provided to generate_signals.\")\n",
        "            return df\n",
        "\n",
        "        # Calculate indicators\n",
        "        df['RSI'] = calculate_rsi(df, self.rsi_period)\n",
        "        df[f'SMA_{self.short_ma_period}'] = calculate_sma(df, self.short_ma_period)\n",
        "        df[f'SMA_{self.long_ma_period}'] = calculate_sma(df, self.long_ma_period)\n",
        "\n",
        "        df['Signal'] = 0 # Default to hold signal\n",
        "        df['Position'] = 0 # 1 for Long, 0 for Flat (no position)\n",
        "\n",
        "        # Drop NaN values introduced by indicator calculations before generating signals\n",
        "        df.dropna(subset=['RSI', f'SMA_{self.short_ma_period}', f'SMA_{self.long_ma_period}'], inplace=True)\n",
        "        if df.empty:\n",
        "            logger.warning(\"DataFrame became empty after dropping NaNs for indicator calculations.\")\n",
        "            return df\n",
        "\n",
        "        # Buy condition: RSI < 30 AND 20-DMA crosses above 50-DMA\n",
        "        # Check current day's MA relationship and previous day's relationship for a crossover\n",
        "        buy_condition = (df['RSI'] < self.rsi_buy_threshold) & \\\n",
        "                        (df[f'SMA_{self.short_ma_period}'] > df[f'SMA_{self.long_ma_period}']) & \\\n",
        "                        (df[f'SMA_{self.short_ma_period}'].shift(1) <= df[f'SMA_{self.long_ma_period}'].shift(1))\n",
        "\n",
        "        # Sell condition: 20-DMA crosses below 50-DMA (as a simple exit)\n",
        "        sell_condition = (df[f'SMA_{self.short_ma_period}'] < df[f'SMA_{self.long_ma_period}']) & \\\n",
        "                         (df[f'SMA_{self.short_ma_period}'].shift(1) >= df[f'SMA_{self.long_ma_period}'].shift(1))\n",
        "\n",
        "        # Apply signals\n",
        "        df.loc[buy_condition, 'Signal'] = 1\n",
        "        df.loc[sell_condition, 'Signal'] = -1\n",
        "\n",
        "        # Determine position based on signals\n",
        "        # This simulates opening a position on a buy signal and closing on a sell signal.\n",
        "        # It's a simplified approach for backtesting.\n",
        "        current_position = 0\n",
        "        position_list = []\n",
        "        for i in range(len(df)):\n",
        "            if df['Signal'].iloc[i] == 1: # Buy signal\n",
        "                current_position = 1 # Go long\n",
        "            elif df['Signal'].iloc[i] == -1: # Sell signal\n",
        "                current_position = 0 # Close position (go flat)\n",
        "            position_list.append(current_position)\n",
        "\n",
        "        df['Position'] = position_list\n",
        "        logger.info(\"Signals generated successfully.\")\n",
        "        return df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4kVEa2ZTdx7",
        "outputId": "7b8fc699-88f3-40f6-f043-4094b355d534"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/algo_trading_prototype/strategy/trading_strategy.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SIMULATES TRADING STRATEGY BASED ON HISTORICAL DATA"
      ],
      "metadata": {
        "id": "5jwlAbAvT2hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 9: backtester/backtester.py\n",
        "%%writefile /content/drive/MyDrive/algo_trading_prototype/backtester/backtester.py\n",
        "# backtester/backtester.py\n",
        "\n",
        "import pandas as pd\n",
        "import logging\n",
        "from strategy.trading_strategy import TradingStrategy\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class Backtester:\n",
        "    def __init__(self):\n",
        "        self.strategy = TradingStrategy()\n",
        "        self.trade_log = [] # To store details of each trade\n",
        "\n",
        "    def run_backtest(self, symbol: str, historical_data: pd.DataFrame) -> dict:\n",
        "        \"\"\"\n",
        "        Runs a backtest on the given historical data for a symbol.\n",
        "        Simulates trades based on signals and calculates P&L.\n",
        "        Parameters:\n",
        "            symbol (str): Stock ticker symbol.\n",
        "            historical_data (pd.DataFrame): DataFrame with historical OHLCV data.\n",
        "        Returns:\n",
        "            dict: Dictionary containing backtest summary (initial/final capital, total P&L)\n",
        "                  and the detailed trade log. Returns empty dict if backtest cannot run.\n",
        "        \"\"\"\n",
        "        if historical_data.empty:\n",
        "            logger.warning(f\"No historical data provided for backtesting {symbol}.\")\n",
        "            return {}\n",
        "\n",
        "        df = historical_data.copy()\n",
        "        df = self.strategy.generate_signals(df)\n",
        "\n",
        "        # Filter out NaN rows that resulted from indicator calculations for cleaner iteration\n",
        "        df.dropna(subset=['RSI', 'Signal', 'Position'], inplace=True)\n",
        "        if df.empty:\n",
        "            logger.warning(f\"Data for {symbol} became empty after dropping NaNs for signals. Cannot backtest.\")\n",
        "            return {}\n",
        "\n",
        "        initial_capital = 100000 # Example initial capital\n",
        "        current_capital = initial_capital\n",
        "        position_open = False\n",
        "        buy_price = 0\n",
        "        shares_held = 0\n",
        "\n",
        "        # Reset trade_log for each backtest run\n",
        "        self.trade_log = []\n",
        "\n",
        "        for i in range(len(df)):\n",
        "            date = df.index[i]\n",
        "            close_price = df['Close'].iloc[i]\n",
        "            signal = df['Signal'].iloc[i]\n",
        "            current_position_status = df['Position'].iloc[i] # Current position from strategy\n",
        "\n",
        "            # Execute buy trade\n",
        "            if signal == 1 and not position_open: # Buy signal and no open position\n",
        "                shares_to_buy = int(current_capital / close_price) # Buy as many shares as possible\n",
        "                if shares_to_buy > 0:\n",
        "                    buy_price = close_price\n",
        "                    shares_held = shares_to_buy\n",
        "                    current_capital -= (shares_held * buy_price) # Deduct cost\n",
        "                    position_open = True\n",
        "                    self.trade_log.append({\n",
        "                        'Symbol': symbol,\n",
        "                        'Date': date.strftime('%Y-%m-%d'),\n",
        "                        'Type': 'BUY',\n",
        "                        'Price': round(buy_price, 2),\n",
        "                        'Shares': shares_held,\n",
        "                        'Capital_After_Trade': round(current_capital, 2),\n",
        "                        'P&L': 0.0 # P&L is realized on sell\n",
        "                    })\n",
        "                    logger.info(f\"{symbol} - {date.strftime('%Y-%m-%d')}: BUY at {buy_price:.2f} (Shares: {shares_held})\")\n",
        "\n",
        "            # Execute sell trade\n",
        "            elif signal == -1 and position_open: # Sell signal and there is an open position\n",
        "                sell_price = close_price\n",
        "                pnl = (sell_price - buy_price) * shares_held\n",
        "                current_capital += (shares_held * sell_price) # Add proceeds\n",
        "                position_open = False\n",
        "                self.trade_log.append({\n",
        "                    'Symbol': symbol,\n",
        "                    'Date': date.strftime('%Y-%m-%d'),\n",
        "                    'Type': 'SELL',\n",
        "                    'Price': round(sell_price, 2),\n",
        "                    'Shares': shares_held,\n",
        "                    'Capital_After_Trade': round(current_capital, 2),\n",
        "                    'P&L': round(pnl, 2)\n",
        "                })\n",
        "                logger.info(f\"{symbol} - {date.strftime('%Y-%m-%d')}: SELL at {sell_price:.2f} (P&L: {pnl:.2f})\")\n",
        "                buy_price = 0 # Reset buy price\n",
        "                shares_held = 0 # Reset shares held\n",
        "\n",
        "        # If a position is still open at the end of the backtest, close it forcibly\n",
        "        if position_open:\n",
        "            sell_price = df['Close'].iloc[-1]\n",
        "            pnl = (sell_price - buy_price) * shares_held\n",
        "            current_capital += (shares_held * sell_price)\n",
        "            self.trade_log.append({\n",
        "                'Symbol': symbol,\n",
        "                'Date': df.index[-1].strftime('%Y-%m-%d'),\n",
        "                'Type': 'SELL (Forced Exit)', # Indicate a forced exit at end of backtest period\n",
        "                'Price': round(sell_price, 2),\n",
        "                'Shares': shares_held,\n",
        "                'Capital_After_Trade': round(current_capital, 2),\n",
        "                'P&L': round(pnl, 2)\n",
        "            })\n",
        "            logger.info(f\"{symbol} - Forced SELL at {sell_price:.2f} (P&L: {pnl:.2f}) at end of backtest.\")\n",
        "\n",
        "        total_pnl = current_capital - initial_capital\n",
        "        logger.info(f\"Backtest for {symbol} completed. Total P&L: {total_pnl:.2f}\")\n",
        "\n",
        "        return {\n",
        "            'symbol': symbol,\n",
        "            'initial_capital': initial_capital,\n",
        "            'final_capital': current_capital,\n",
        "            'total_pnl': total_pnl,\n",
        "            'trade_log': pd.DataFrame(self.trade_log) # Convert to DataFrame for easier handling\n",
        "        }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wacWdbGATvov",
        "outputId": "14eaee72-42a1-478a-e69a-c30abdcde928"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/algo_trading_prototype/backtester/backtester.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPLEMENTS THE MACHINE LEARNING MODEL FOR NEXT DAY MOVEMENT PREDICTION"
      ],
      "metadata": {
        "id": "v38kah48hg1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/algo_trading_prototype/analytics/ml_predictor.py\n",
        "# analytics/ml_predictor.py\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import logging\n",
        "from strategy.indicators import calculate_rsi, calculate_macd\n",
        "from config import settings\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class MLPredictor:\n",
        "    def __init__(self, model_type: str = 'decision_tree'):\n",
        "        \"\"\"\n",
        "        Initializes the ML Predictor with a specified model type.\n",
        "        Parameters:\n",
        "            model_type (str): 'decision_tree' or 'logistic_regression'.\n",
        "        \"\"\"\n",
        "        if model_type == 'decision_tree':\n",
        "            self.model = DecisionTreeClassifier(random_state=42)\n",
        "        elif model_type == 'logistic_regression':\n",
        "            self.model = LogisticRegression(random_state=42, solver='liblinear')\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported model_type. Choose 'decision_tree' or 'logistic_regression'.\")\n",
        "        self.trained = False # Flag to check if model has been trained\n",
        "\n",
        "\n",
        "    def prepare_data_for_ml(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        # Calculate features\n",
        "        df['RSI'] = calculate_rsi(df, window=settings.RSI_PERIOD)\n",
        "        # Ensure MACD df has a matching index to the main df before merging\n",
        "        macd_df = calculate_macd(df)\n",
        "\n",
        "        # Use pd.merge on the index (which is 'Date')\n",
        "        # Assuming both df and macd_df have a clean DatetimeIndex\n",
        "        df = pd.merge(df, macd_df, left_index=True, right_index=True, how='inner') # Use inner to keep only common dates\n",
        "\n",
        "\n",
        "        # Calculate next-day close movement as target (1 for Up, 0 for Down/No Change)\n",
        "        # Shift(-1) means the close price of the *next* row\n",
        "        df['Next_Day_Close'] = df['Close'].shift(-1)\n",
        "        # Target: 1 if next day's close is higher, 0 otherwise\n",
        "        df['Next_Day_Movement'] = (df['Next_Day_Close'] > df['Close']).astype(int)\n",
        "\n",
        "        # Drop rows with NaN values (due to indicator calculation or Next_Day_Close for last row)\n",
        "        df.dropna(subset=settings.FEATURES + ['Next_Day_Movement'], inplace=True)\n",
        "        return df\n",
        "\n",
        "    def train_model(self, df: pd.DataFrame, features: list, target: str):\n",
        "        \"\"\"\n",
        "        Trains the ML model and evaluates its performance.\n",
        "        Parameters:\n",
        "            df (pd.DataFrame): Prepared DataFrame with features and target.\n",
        "            features (list): List of column names to use as features.\n",
        "            target (str): Name of the target column.\n",
        "        Returns:\n",
        "            tuple: (accuracy, classification_report_string). Returns (0.0, \"\") if training fails.\n",
        "        \"\"\"\n",
        "        if df.empty or not all(col in df.columns for col in features + [target]):\n",
        "            logger.warning(\"Insufficient data or missing columns for ML training.\")\n",
        "            self.trained = False\n",
        "            return 0.0, \"\"\n",
        "\n",
        "        X = df[features]\n",
        "        y = df[target]\n",
        "\n",
        "        # Handle cases where only one class is present in the target variable\n",
        "        if len(y.unique()) < 2:\n",
        "            logger.warning(f\"Only one class present in target variable for ML training (symbol has no 'Up' or 'Down' movements in data). Skipping training.\")\n",
        "            self.trained = False\n",
        "            return 0.0, \"Only one class in target\"\n",
        "\n",
        "        try:\n",
        "            # Stratify ensures that the train/test split maintains the proportion of classes\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "        except ValueError as e:\n",
        "            logger.warning(f\"Could not split data for ML training due to: {e}. Check data balance or size.\")\n",
        "            self.trained = False\n",
        "            return 0.0, str(e)\n",
        "\n",
        "        self.model.fit(X_train, y_train)\n",
        "        y_pred = self.model.predict(X_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        # zero_division=0 prevents warnings when a class has no predicted samples\n",
        "        report = classification_report(y_test, y_pred, zero_division=0)\n",
        "\n",
        "        logger.info(f\"ML Model Training Complete (Model: {type(self.model).__name__}):\")\n",
        "        logger.info(f\"Accuracy: {accuracy:.4f}\")\n",
        "        logger.info(f\"Classification Report:\\n{report}\")\n",
        "        self.trained = True\n",
        "        return accuracy, report\n",
        "\n",
        "    def predict_next_day_movement(self, latest_data_df: pd.DataFrame, features: list) -> int:\n",
        "        \"\"\"\n",
        "        Predicts the next day's movement (1 for Up, 0 for Down/No Change) based on the trained model.\n",
        "        `latest_data_df` should contain enough historical rows to calculate required indicators\n",
        "        for the *last* data point.\n",
        "        Parameters:\n",
        "            latest_data_df (pd.DataFrame): DataFrame containing recent historical OHLCV data.\n",
        "                                           Should include enough rows to calculate all features.\n",
        "            features (list): List of feature column names used during training.\n",
        "        Returns:\n",
        "            int: 1 for 'Up', 0 for 'Down/No Change', -1 if prediction cannot be made (e.g., not trained, no data).\n",
        "        \"\"\"\n",
        "        if not self.trained:\n",
        "            logger.warning(\"ML model is not trained. Cannot make prediction.\")\n",
        "            return -1\n",
        "\n",
        "        if latest_data_df.empty:\n",
        "            logger.warning(\"Empty DataFrame provided for ML prediction.\")\n",
        "            return -1\n",
        "\n",
        "        # Prepare the data to get the latest features, similar to training data prep\n",
        "        processed_df = self.prepare_data_for_ml(latest_data_df.copy())\n",
        "        if processed_df.empty:\n",
        "            logger.warning(\"Could not process latest data for prediction after feature engineering.\")\n",
        "            return -1\n",
        "\n",
        "        # The last row of `processed_df` contains the latest calculated features\n",
        "        # and its 'Next_Day_Movement' would be NaN (which is fine for prediction input)\n",
        "        try:\n",
        "            latest_features_row = processed_df[features].iloc[-1]\n",
        "            # Reshape for prediction (sklearn expects 2D array, even for a single sample)\n",
        "            prediction_input = latest_features_row.to_frame().T\n",
        "            prediction = self.model.predict(prediction_input)[0]\n",
        "            logger.info(f\"Next day movement prediction: {'Up' if prediction == 1 else 'Down/No Change'}\")\n",
        "            return int(prediction) # You had a stray comment/code here, moved this line up\n",
        "        except IndexError:\n",
        "            logger.warning(\"Not enough data points in processed_df for prediction after feature engineering.\")\n",
        "            return -1\n",
        "        except KeyError as e:\n",
        "            logger.warning(f\"Missing feature for prediction: {e}. Check FEATURES in settings.py and data preparation.\")\n",
        "            return -1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpi8UXWfT0pv",
        "outputId": "aef88d3a-4313-4a39-b990-f8708d886dc1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/algo_trading_prototype/analytics/ml_predictor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MANAGES INTERACTIONS WITH GOOGLE SHEETS"
      ],
      "metadata": {
        "id": "LZyv5x8Mh2zH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/algo_trading_prototype/sheets/google_sheets_manager.py\n",
        "# sheets/google_sheets_manager.py\n",
        "import gspread\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "from config import settings # Make sure settings.TELEGRAM_ALERTS_WORKSHEET_NAME is defined here\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class GoogleSheetsManager:\n",
        "    def __init__(self):\n",
        "        self.spreadsheet = None\n",
        "        try:\n",
        "            credentials_path = \"/content/drive/MyDrive/algo_trading_prototype/credentials.json\"\n",
        "\n",
        "            if not os.path.exists(credentials_path):\n",
        "                logger.error(f\"FATAL: credentials.json NOT FOUND at {credentials_path}\")\n",
        "                raise FileNotFoundError(f\"credentials.json not found at {credentials_path}\")\n",
        "\n",
        "            logger.info(f\"Attempting to load service account from: {credentials_path}\")\n",
        "            gc_temp = gspread.service_account(filename=credentials_path)\n",
        "\n",
        "            if not isinstance(gc_temp, gspread.Client):\n",
        "                logger.error(f\"FATAL: gspread.service_account did NOT return a Client object. Type received: {type(gc_temp)}\")\n",
        "                raise TypeError(\"gspread.service_account did not return a gspread.Client object.\")\n",
        "            self.gc = gc_temp\n",
        "\n",
        "            logger.info(\"gspread service account client created successfully.\")\n",
        "\n",
        "            self.spreadsheet = self.gc.open_by_id(settings.GOOGLE_SHEET_ID)\n",
        "            logger.info(f\"Successfully connected to Google Sheet: {self.spreadsheet.title}\")\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            logger.error(f\"Error: {e}\\nPlease ensure your 'credentials.json' is correctly placed and named.\")\n",
        "            self.spreadsheet = None\n",
        "        except gspread.exceptions.SpreadsheetNotFound:\n",
        "            logger.error(f\"Google Sheet with ID '{settings.GOOGLE_SHEET_ID}' not found or you don't have access. \"\n",
        "                         f\"Please check the ID and ensure it's shared with your service account email.\")\n",
        "            self.spreadsheet = None\n",
        "        except Exception as e: # Catch all other exceptions\n",
        "            logger.error(f\"General error connecting to Google Sheets: {e}\")\n",
        "            logger.error(f\"Please ensure:\\n\"\n",
        "                         f\"1. Google Sheets API is enabled in your GCP project.\\n\"\n",
        "                         f\"2. The Google Sheet (ID: {settings.GOOGLE_SHEET_ID}) is shared with the service account email (found in 'credentials.json').\")\n",
        "            self.spreadsheet = None\n",
        "\n",
        "    def get_worksheet(self, worksheet_name: str):\n",
        "        if self.spreadsheet:\n",
        "            try:\n",
        "                return self.spreadsheet.worksheet(worksheet_name)\n",
        "            except gspread.exceptions.WorksheetNotFound:\n",
        "                logger.error(f\"Worksheet '{worksheet_name}' not found in the Google Sheet.\")\n",
        "                return None\n",
        "        return None\n",
        "\n",
        "    def read_data(self, worksheet_name: str):\n",
        "        worksheet = self.get_worksheet(worksheet_name)\n",
        "        if worksheet:\n",
        "            try:\n",
        "                data = worksheet.get_all_values()\n",
        "                if not data:\n",
        "                    return pd.DataFrame()\n",
        "                df = pd.DataFrame(data[1:], columns=data[0])\n",
        "                return df\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error reading data from worksheet '{worksheet_name}': {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    def write_data(self, worksheet_name: str, df: pd.DataFrame, clear_existing: bool = True):\n",
        "        worksheet = self.get_worksheet(worksheet_name)\n",
        "        if worksheet:\n",
        "            try:\n",
        "                if clear_existing:\n",
        "                    worksheet.clear()\n",
        "                # Prepare data including headers\n",
        "                data_to_write = [df.columns.values.tolist()] + df.values.tolist()\n",
        "                worksheet.update(data_to_write)\n",
        "                logger.info(f\"Successfully wrote data to worksheet '{worksheet_name}'.\")\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error writing data to worksheet '{worksheet_name}': {e}\")\n",
        "        return False\n",
        "\n",
        "    def update_summary_pnl(self, pnl_results: dict):\n",
        "        \"\"\"\n",
        "        Updates the 'Summary PnL' sheet with the P&L for each symbol.\n",
        "        Parameters:\n",
        "            pnl_results (dict): A dictionary with symbol as key and P&L as value.\n",
        "        \"\"\"\n",
        "        if not self.spreadsheet:\n",
        "            logger.warning(\"Google Sheets connection not established. Cannot update summary P&L.\")\n",
        "            return\n",
        "\n",
        "        worksheet_name = settings.GOOGLE_SHEET_PNL_WORKSHEET_NAME # This should be 'Summary PnL'\n",
        "        try:\n",
        "            worksheet = self.spreadsheet.worksheet(worksheet_name)\n",
        "        except gspread.exceptions.WorksheetNotFound:\n",
        "            logger.warning(f\"Worksheet '{worksheet_name}' not found. Creating it...\")\n",
        "            worksheet = self.spreadsheet.add_worksheet(title=worksheet_name, rows=\"100\", cols=\"20\")\n",
        "            # Add headers after creating\n",
        "            worksheet.update('A1', [['Symbol', 'Total PnL']])\n",
        "            logger.info(f\"Worksheet '{worksheet_name}' created successfully.\")\n",
        "\n",
        "        try:\n",
        "            # Prepare data for update\n",
        "            data_to_write = [['Symbol', 'Total PnL']]\n",
        "            for symbol, pnl in pnl_results.items():\n",
        "                data_to_write.append([symbol, pnl])\n",
        "\n",
        "            # Clear existing data and update. Starting from A1 includes headers.\n",
        "            worksheet.update('A1', data_to_write)\n",
        "            logger.info(f\"Successfully updated summary P&L in Google Sheet '{worksheet_name}'.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error updating summary P&L in Google Sheet '{worksheet_name}': {e}\")\n",
        "\n",
        "    # --- ADD THIS NEW METHOD: get_signal_alerts ---\n",
        "    def get_signal_alerts(self, symbol: str, date: str, signal_type: str, current_price: float) -> str:\n",
        "        \"\"\"\n",
        "        Reads alert configurations from the 'Alerts' worksheet and generates an alert message\n",
        "        if conditions are met.\n",
        "        Parameters:\n",
        "            symbol (str): The stock symbol.\n",
        "            date (str): Current date (YYYY-MM-DD).\n",
        "            signal_type (str): The generated trading signal (e.g., 'BUY', 'SELL', 'HOLD').\n",
        "            current_price (float): The current closing price of the stock.\n",
        "        Returns:\n",
        "            str: An alert message if conditions are met, otherwise an empty string.\n",
        "        \"\"\"\n",
        "        if not self.spreadsheet:\n",
        "            logger.warning(\"Google Sheets connection not established. Cannot fetch alert configurations.\")\n",
        "            return \"\"\n",
        "\n",
        "        alert_worksheet_name = settings.TELEGRAM_ALERTS_WORKSHEET_NAME # Assumed to be 'Alerts'\n",
        "        try:\n",
        "            alerts_df = self.read_data(alert_worksheet_name)\n",
        "            if alerts_df.empty:\n",
        "                logger.info(f\"No alert configurations found in '{alert_worksheet_name}' worksheet.\")\n",
        "                return \"\"\n",
        "\n",
        "            # Ensure columns are as expected and convert numeric types\n",
        "            required_cols = ['Symbol', 'SignalType', 'PriceThreshold', 'Direction', 'Enabled']\n",
        "            if not all(col in alerts_df.columns for col in required_cols):\n",
        "                logger.warning(f\"Alerts worksheet '{alert_worksheet_name}' missing required columns. \"\n",
        "                               f\"Expected: {required_cols}. Found: {alerts_df.columns.tolist()}\")\n",
        "                return \"\"\n",
        "\n",
        "            alerts_df['PriceThreshold'] = pd.to_numeric(alerts_df['PriceThreshold'], errors='coerce')\n",
        "            alerts_df['Enabled'] = alerts_df['Enabled'].astype(str).str.lower().str.strip() == 'true'\n",
        "\n",
        "            alert_message = \"\"\n",
        "            # Filter for enabled alerts relevant to the current symbol\n",
        "            relevant_alerts = alerts_df[(alerts_df['Symbol'] == symbol) & (alerts_df['Enabled'])]\n",
        "\n",
        "            for _, row in relevant_alerts.iterrows():\n",
        "                alert_signal = row['SignalType'].strip().upper()\n",
        "                price_threshold = row['PriceThreshold']\n",
        "                direction = row['Direction'].strip().lower() # 'above' or 'below'\n",
        "\n",
        "                # Check for signal match\n",
        "                if signal_type.upper() == alert_signal:\n",
        "                    # Check for price threshold conditions (if PriceThreshold is not NaN)\n",
        "                    price_condition_met = False\n",
        "                    if pd.notna(price_threshold):\n",
        "                        if direction == 'above' and current_price > price_threshold:\n",
        "                            price_condition_met = True\n",
        "                        elif direction == 'below' and current_price < price_threshold:\n",
        "                            price_condition_met = True\n",
        "                        elif direction == 'any': # Alert on signal regardless of price threshold if 'any' is specified\n",
        "                             price_condition_met = True\n",
        "                    else: # If no price threshold, then signal alone is enough\n",
        "                        price_condition_met = True\n",
        "\n",
        "                    if price_condition_met:\n",
        "                        alert_message += (\n",
        "                            f\"🔔 ALERT for {symbol} on {date}:\\n\"\n",
        "                            f\"Signal: {signal_type.upper()}\\n\"\n",
        "                            f\"Current Price: {current_price:.2f}\\n\"\n",
        "                        )\n",
        "                        if pd.notna(price_threshold):\n",
        "                             alert_message += f\"Threshold: {price_threshold:.2f} ({direction})\\n\"\n",
        "                        alert_message += \"--- Sent by Algo-Trading Prototype ---\\n\\n\"\n",
        "\n",
        "            return alert_message.strip() # Remove trailing newlines/spaces\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching or processing signal alerts from Google Sheet: {e}\")\n",
        "            return \"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a355eEPdh02M",
        "outputId": "ea8f48db-a123-487d-9f4c-e187e349f24e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/algo_trading_prototype/sheets/google_sheets_manager.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HANDLES TELEGRAM MESSAGES"
      ],
      "metadata": {
        "id": "5VbUITMXiVbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: utils/alerts.py\n",
        "%%writefile /content/drive/MyDrive/algo_trading_prototype/utils/alerts.py\n",
        "# utils/alerts.py\n",
        "\n",
        "import requests\n",
        "import logging\n",
        "from config import settings # Import settings module directly\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def send_telegram_message(message: str):\n",
        "    \"\"\"\n",
        "    Sends a message to a specified Telegram chat.\n",
        "    Requires TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID from config/settings.py.\n",
        "    Parameters:\n",
        "        message (str): The text message to send.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        bot_token = settings.TELEGRAM_BOT_TOKEN\n",
        "        chat_id = settings.TELEGRAM_CHAT_ID\n",
        "    except AttributeError:\n",
        "        logger.error(\"Telegram bot token or chat ID not found in settings.py. Skipping Telegram alert.\")\n",
        "        return\n",
        "\n",
        "    if not bot_token or not chat_id:\n",
        "        logger.warning(\"Telegram BOT_TOKEN or CHAT_ID is not configured. Skipping alert.\")\n",
        "        return\n",
        "\n",
        "    url = f\"https://api.telegram.org/bot{bot_token}/sendMessage\"\n",
        "    payload = {\n",
        "        \"chat_id\": chat_id,\n",
        "        \"text\": message,\n",
        "        \"parse_mode\": \"Markdown\" # Allows basic markdown formatting in message\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(url, json=payload)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        logger.info(\"Telegram message sent successfully.\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error sending Telegram message: {e}\")"
      ],
      "metadata": {
        "id": "mIYasGnZiKRS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c0e589-ccda-4032-a980-52f786c2e5ea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/algo_trading_prototype/utils/alerts.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PLACEHOLDER FOR THIS PROTOTYPE"
      ],
      "metadata": {
        "id": "WTkuaMJRklL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: analytics/portfolio_analytics.py\n",
        "%%writefile /content/drive/MyDrive/algo_trading_prototype/analytics/portfolio_analytics.py\n",
        "# analytics/portfolio_analytics.py\n",
        "\n",
        "# This file is a placeholder. Its functionalities (P&L, Win Ratio)\n",
        "# are integrated directly into Backtester and GoogleSheetsManager for simplicity.\n",
        "# For more complex analytics (e.g., Sharpe Ratio, Max Drawdown), you would\n",
        "# implement them here."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2COS9PskbBx",
        "outputId": "81949d6f-f1fa-49d7-ac4b-3adae2976177"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/algo_trading_prototype/analytics/portfolio_analytics.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: main.py\n",
        "%%writefile /content/drive/MyDrive/algo_trading_prototype/main.py\n",
        "# main.py\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Define the project root directly within main.py\n",
        "# This ensures that main.py itself knows where the root of your project is,\n",
        "# regardless of how it's executed, which helps Python find your modules.\n",
        "project_root = '/content/drive/MyDrive/algo_trading_prototype'\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "    # Note: If you modify this or any other module, you might need to\n",
        "    # restart your Colab runtime to ensure changes are picked up due to Python's module caching.\n",
        "\n",
        "# Now, import modules using their top-level package names relative to project_root\n",
        "from config import settings\n",
        "from utils import logger\n",
        "from data import data_fetcher\n",
        "from backtester import backtester\n",
        "from analytics import ml_predictor\n",
        "from sheets import google_sheets_manager\n",
        "from utils import alerts # Bonus: Telegram alerts\n",
        "\n",
        "# Setup logging. This needs to be called to configure loggers globally.\n",
        "logger.setup_logging()\n",
        "main_logger = logging.getLogger(__name__) # Get a specific logger for the main script\n",
        "\n",
        "def run_algo_prototype():\n",
        "    \"\"\"\n",
        "    Main function to run the algo-trading prototype.\n",
        "    Orchestrates data fetching, strategy application, backtesting,\n",
        "    ML prediction, and Google Sheets logging.\n",
        "    \"\"\"\n",
        "    main_logger.info(\"Starting Algo-Trading Prototype...\")\n",
        "\n",
        "    # Initialize components\n",
        "    sheets_manager = google_sheets_manager.GoogleSheetsManager()\n",
        "    backtester_instance = backtester.Backtester()\n",
        "    ml_predictor_instance = ml_predictor.MLPredictor(model_type='decision_tree') # Can be 'logistic_regression'\n",
        "\n",
        "    all_trade_logs = pd.DataFrame() # To aggregate trade logs from all symbols\n",
        "    symbol_pnl_results = {} # To store P&L summary per symbol\n",
        "    ml_accuracies = {} # To store ML model accuracies per symbol\n",
        "\n",
        "    # --- 1. Data Ingestion & ML Model Training ---\n",
        "    main_logger.info(f\"Fetching historical data for {settings.STOCK_SYMBOLS} for {settings.BACKTEST_DURATION_MONTHS} months...\")\n",
        "    historical_data = data_fetcher.get_historical_data(settings.STOCK_SYMBOLS, settings.BACKTEST_DURATION_MONTHS)\n",
        "\n",
        "    if not historical_data:\n",
        "        main_logger.error(\"No historical data fetched. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Train ML model for each stock using its historical data\n",
        "    main_logger.info(\"Preparing data and training ML model for each stock...\")\n",
        "    for symbol, df in historical_data.items():\n",
        "        if not df.empty:\n",
        "            ml_data = ml_predictor_instance.prepare_data_for_ml(df.copy())\n",
        "            if not ml_data.empty:\n",
        "                accuracy, _ = ml_predictor_instance.train_model(ml_data, settings.FEATURES, settings.TARGET)\n",
        "                ml_accuracies[symbol] = accuracy\n",
        "            else:\n",
        "                main_logger.warning(f\"Could not prepare ML data for {symbol}.\")\n",
        "        else:\n",
        "            main_logger.warning(f\"No data for {symbol} to train ML model.\")\n",
        "\n",
        "    main_logger.info(f\"ML Model Accuracies: {ml_accuracies}\")\n",
        "\n",
        "    # --- 2. Run Backtest for each stock ---\n",
        "    for symbol, df in historical_data.items():\n",
        "        if df.empty:\n",
        "            main_logger.warning(f\"Skipping backtest for {symbol} due to no data.\")\n",
        "            continue\n",
        "\n",
        "        main_logger.info(f\"Running backtest for {symbol}...\")\n",
        "        results = backtester_instance.run_backtest(symbol, df.copy())\n",
        "        if results:\n",
        "            symbol_pnl_results[symbol] = {\n",
        "                'initial_capital': results['initial_capital'],\n",
        "                'final_capital': results['final_capital'],\n",
        "                'total_pnl': results['total_pnl']\n",
        "            }\n",
        "            if not results['trade_log'].empty:\n",
        "                # Concatenate trade logs from each symbol into one DataFrame\n",
        "                all_trade_logs = pd.concat([all_trade_logs, results['trade_log']], ignore_index=True)\n",
        "        else:\n",
        "            main_logger.warning(f\"Backtest for {symbol} yielded no results.\")\n",
        "\n",
        "    # --- 3. Google Sheets Automation ---\n",
        "    # Log all aggregated trade signals\n",
        "    if not all_trade_logs.empty:\n",
        "        main_logger.info(\"Logging trade signals to Google Sheets...\")\n",
        "        sheets_manager.log_trade_signals(all_trade_logs)\n",
        "        # Update win ratio based on all logs\n",
        "        sheets_manager.update_win_ratio(all_trade_logs)\n",
        "    else:\n",
        "        main_logger.info(\"No trade logs to write to Google Sheets.\")\n",
        "\n",
        "    # Update summary P&L\n",
        "    if symbol_pnl_results:\n",
        "        main_logger.info(\"Updating summary P&L in Google Sheets...\")\n",
        "        sheets_manager.update_summary_pnl(symbol_pnl_results)\n",
        "    else:\n",
        "        main_logger.info(\"No summary P&L to write to Google Sheets.\")\n",
        "\n",
        "    # --- 4. Generate Current Buy/Sell Signals and ML Predictions ---\n",
        "    main_logger.info(\"Generating current (latest date) buy/sell signals and ML predictions...\")\n",
        "    for symbol, df in historical_data.items():\n",
        "        if df.empty:\n",
        "            main_logger.warning(f\"Cannot generate signal for {symbol}: no data.\")\n",
        "            continue\n",
        "\n",
        "        # Get enough historical data points to calculate all indicators for the latest day\n",
        "        required_rows_for_indicators = max(settings.RSI_PERIOD, settings.SHORT_MA_PERIOD, settings.LONG_MA_PERIOD) + 1 # +1 for current day's signal\n",
        "        latest_data_for_signal = df.tail(required_rows_for_indicators).copy()\n",
        "\n",
        "        if latest_data_for_signal.empty:\n",
        "            main_logger.warning(f\"Not enough recent data for {symbol} to generate current signal after indicator calculation.\")\n",
        "            continue\n",
        "\n",
        "        # Re-run strategy on the latest data to get the current day's signal\n",
        "        processed_latest_data = backtester_instance.strategy.generate_signals(latest_data_for_signal)\n",
        "\n",
        "        # Check if the last row (current day) has valid signal/data\n",
        "        if processed_latest_data.empty or processed_latest_data.iloc[-1].isnull().any():\n",
        "            main_logger.warning(f\"Could not generate clean signal for {symbol} for the latest date after processing.\")\n",
        "            current_signal = 0 # Default to hold if signal is unreliable\n",
        "            current_close = df['Close'].iloc[-1] if not df.empty else 0\n",
        "            current_date = df.index[-1].strftime('%Y-%m-%d') if not df.empty else \"N/A\"\n",
        "        else:\n",
        "            current_signal = processed_latest_data['Signal'].iloc[-1]\n",
        "            current_close = processed_latest_data['Close'].iloc[-1]\n",
        "            current_date = processed_latest_data.index[-1].strftime('%Y-%m-%d')\n",
        "\n",
        "        signal_type = \"HOLD\"\n",
        "        if current_signal == 1:\n",
        "            signal_type = \"BUY\"\n",
        "        elif current_signal == -1:\n",
        "            signal_type = \"SELL\"\n",
        "\n",
        "        main_logger.info(f\"[{symbol}] Latest Strategy Signal ({current_date}): {signal_type} at {current_close:.2f}\")\n",
        "\n",
        "        # ML prediction for next day movement\n",
        "        # Need enough data points for ML features (RSI, MACD, Volume) for the current day\n",
        "        # `prepare_data_for_ml` handles dropping NaNs, so pass a sufficient window of recent data\n",
        "        ml_prediction_input_data = df.tail(max(settings.RSI_PERIOD, 26) + 2).copy() # MACD needs up to 26 periods + 1 for next day shift\n",
        "        if not ml_prediction_input_data.empty and ml_predictor_instance.trained:\n",
        "            next_day_pred = ml_predictor_instance.predict_next_day_movement(ml_prediction_input_data, settings.FEATURES)\n",
        "            ml_pred_text = \"Up\" if next_day_pred == 1 else (\"Down/No Change\" if next_day_pred == 0 else \"N/A - Prediction Failed\")\n",
        "            main_logger.info(f\"[{symbol}] Next Day ML Prediction: {ml_pred_text}\")\n",
        "        else:\n",
        "            ml_pred_text = \"N/A - Model Not Trained or Insufficient Data\"\n",
        "            main_logger.warning(f\"[{symbol}] {ml_pred_text}\")\n",
        "\n",
        "        # Bonus: Telegram Alert Integration\n",
        "        if current_signal != 0 or next_day_pred != -1: # Alert for strategy signals or if ML made a valid prediction\n",
        "            alert_message = sheets_manager.get_signal_alerts(symbol, current_date, signal_type, current_close)\n",
        "            alert_message += f\"\\nML Prediction (Next Day): {ml_pred_text}\"\n",
        "            alerts.send_telegram_message(alert_message)\n",
        "\n",
        "    main_logger.info(\"Algo-Trading Prototype finished.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_algo_prototype()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jna6aUWBkkZb",
        "outputId": "676cc5fc-ffed-4d4b-cc41-e658ad24871e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/algo_trading_prototype/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO RUN THE MAIN FILE"
      ],
      "metadata": {
        "id": "3LuAxRfmm5pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute main.py\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Define your project root in Google Drive\n",
        "project_root = '/content/drive/MyDrive/algo_trading_prototype'\n",
        "\n",
        "# Change the current working directory to the project root\n",
        "# This is generally good practice when running a main script from a package.\n",
        "os.chdir(project_root)\n",
        "print(f\"Changed current working directory to: {os.getcwd()}\")\n",
        "\n",
        "# Run your main script.\n",
        "# Because sys.path is handled within main.py, it should now find its modules.\n",
        "!python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJrmhP6Ik53D",
        "outputId": "bb864aa1-3303-4228-f386-f8643d83222e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Changed current working directory to: /content/drive/MyDrive/algo_trading_prototype\n",
            "2025-06-29 13:31:01,909 - __main__ - INFO - Starting Algo-Trading Prototype...\n",
            "2025-06-29 13:31:01,911 - sheets.google_sheets_manager - INFO - Attempting to load service account from: /content/drive/MyDrive/algo_trading_prototype/credentials.json\n",
            "2025-06-29 13:31:01,977 - sheets.google_sheets_manager - INFO - gspread service account client created successfully.\n",
            "2025-06-29 13:31:01,978 - sheets.google_sheets_manager - ERROR - General error connecting to Google Sheets: 'Client' object has no attribute 'open_by_id'\n",
            "2025-06-29 13:31:01,978 - sheets.google_sheets_manager - ERROR - Please ensure:\n",
            "1. Google Sheets API is enabled in your GCP project.\n",
            "2. The Google Sheet (ID: 1W8nP1H7oIKwdy7m5dGii0lZcf_cKxn0mPSdzhAddm80) is shared with the service account email (found in 'credentials.json').\n",
            "2025-06-29 13:31:01,980 - __main__ - INFO - Fetching historical data for ['RELIANCE.NS', 'TCS.NS', 'HDFCBANK.NS'] for 6 months...\n",
            "/content/drive/MyDrive/algo_trading_prototype/data/data_fetcher.py:16: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(symbol, start=start_date, end=end_date, progress=False)\n",
            "2025-06-29 13:31:02,275 - data.data_fetcher - INFO - Flattened MultiIndex columns for RELIANCE.NS. Raw flattened columns: ['Close', 'High', 'Low', 'Open', 'Volume']\n",
            "2025-06-29 13:31:02,276 - data.data_fetcher - INFO - Successfully fetched and prepared historical data for RELIANCE.NS. Final columns: ['Open', 'High', 'Low', 'Close', 'Volume']\n",
            "/content/drive/MyDrive/algo_trading_prototype/data/data_fetcher.py:16: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(symbol, start=start_date, end=end_date, progress=False)\n",
            "2025-06-29 13:31:02,414 - data.data_fetcher - INFO - Flattened MultiIndex columns for TCS.NS. Raw flattened columns: ['Close', 'High', 'Low', 'Open', 'Volume']\n",
            "2025-06-29 13:31:02,416 - data.data_fetcher - INFO - Successfully fetched and prepared historical data for TCS.NS. Final columns: ['Open', 'High', 'Low', 'Close', 'Volume']\n",
            "/content/drive/MyDrive/algo_trading_prototype/data/data_fetcher.py:16: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(symbol, start=start_date, end=end_date, progress=False)\n",
            "2025-06-29 13:31:02,574 - data.data_fetcher - INFO - Flattened MultiIndex columns for HDFCBANK.NS. Raw flattened columns: ['Close', 'High', 'Low', 'Open', 'Volume']\n",
            "2025-06-29 13:31:02,575 - data.data_fetcher - INFO - Successfully fetched and prepared historical data for HDFCBANK.NS. Final columns: ['Open', 'High', 'Low', 'Close', 'Volume']\n",
            "2025-06-29 13:31:02,575 - __main__ - INFO - Preparing data and training ML model for each stock...\n",
            "2025-06-29 13:31:02,596 - analytics.ml_predictor - INFO - ML Model Training Complete (Model: DecisionTreeClassifier):\n",
            "2025-06-29 13:31:02,597 - analytics.ml_predictor - INFO - Accuracy: 0.4500\n",
            "2025-06-29 13:31:02,597 - analytics.ml_predictor - INFO - Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.33      0.35         9\n",
            "           1       0.50      0.55      0.52        11\n",
            "\n",
            "    accuracy                           0.45        20\n",
            "   macro avg       0.44      0.44      0.44        20\n",
            "weighted avg       0.44      0.45      0.45        20\n",
            "\n",
            "2025-06-29 13:31:02,618 - analytics.ml_predictor - INFO - ML Model Training Complete (Model: DecisionTreeClassifier):\n",
            "2025-06-29 13:31:02,619 - analytics.ml_predictor - INFO - Accuracy: 0.5000\n",
            "2025-06-29 13:31:02,619 - analytics.ml_predictor - INFO - Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.36      0.44        11\n",
            "           1       0.46      0.67      0.55         9\n",
            "\n",
            "    accuracy                           0.50        20\n",
            "   macro avg       0.52      0.52      0.49        20\n",
            "weighted avg       0.52      0.50      0.49        20\n",
            "\n",
            "2025-06-29 13:31:02,646 - analytics.ml_predictor - INFO - ML Model Training Complete (Model: DecisionTreeClassifier):\n",
            "2025-06-29 13:31:02,647 - analytics.ml_predictor - INFO - Accuracy: 0.6500\n",
            "2025-06-29 13:31:02,647 - analytics.ml_predictor - INFO - Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.44      0.53         9\n",
            "           1       0.64      0.82      0.72        11\n",
            "\n",
            "    accuracy                           0.65        20\n",
            "   macro avg       0.65      0.63      0.63        20\n",
            "weighted avg       0.65      0.65      0.64        20\n",
            "\n",
            "2025-06-29 13:31:02,647 - __main__ - INFO - ML Model Accuracies: {'RELIANCE.NS': 0.45, 'TCS.NS': 0.5, 'HDFCBANK.NS': 0.65}\n",
            "2025-06-29 13:31:02,648 - __main__ - INFO - Running backtest for RELIANCE.NS...\n",
            "2025-06-29 13:31:02,658 - strategy.trading_strategy - INFO - Signals generated successfully.\n",
            "2025-06-29 13:31:02,663 - backtester.backtester - INFO - Backtest for RELIANCE.NS completed. Total P&L: 0.00\n",
            "2025-06-29 13:31:02,664 - __main__ - INFO - Running backtest for TCS.NS...\n",
            "2025-06-29 13:31:02,674 - strategy.trading_strategy - INFO - Signals generated successfully.\n",
            "2025-06-29 13:31:02,678 - backtester.backtester - INFO - Backtest for TCS.NS completed. Total P&L: 0.00\n",
            "2025-06-29 13:31:02,679 - __main__ - INFO - Running backtest for HDFCBANK.NS...\n",
            "2025-06-29 13:31:02,689 - strategy.trading_strategy - INFO - Signals generated successfully.\n",
            "2025-06-29 13:31:02,694 - backtester.backtester - INFO - Backtest for HDFCBANK.NS completed. Total P&L: 0.00\n",
            "2025-06-29 13:31:02,696 - __main__ - INFO - No trade logs to write to Google Sheets.\n",
            "2025-06-29 13:31:02,696 - __main__ - INFO - Updating summary P&L in Google Sheets...\n",
            "2025-06-29 13:31:02,696 - sheets.google_sheets_manager - WARNING - Google Sheets connection not established. Cannot update summary P&L.\n",
            "2025-06-29 13:31:02,696 - __main__ - INFO - Generating current (latest date) buy/sell signals and ML predictions...\n",
            "2025-06-29 13:31:02,703 - strategy.trading_strategy - INFO - Signals generated successfully.\n",
            "2025-06-29 13:31:02,704 - __main__ - INFO - [RELIANCE.NS] Latest Strategy Signal (2025-06-27): HOLD at 1515.40\n",
            "2025-06-29 13:31:02,713 - analytics.ml_predictor - INFO - Next day movement prediction: Up\n",
            "2025-06-29 13:31:02,715 - __main__ - INFO - [RELIANCE.NS] Next Day ML Prediction: Up\n",
            "2025-06-29 13:31:02,715 - sheets.google_sheets_manager - WARNING - Google Sheets connection not established. Cannot fetch alert configurations.\n",
            "2025-06-29 13:31:03,381 - utils.alerts - INFO - Telegram message sent successfully.\n",
            "2025-06-29 13:31:03,407 - strategy.trading_strategy - INFO - Signals generated successfully.\n",
            "2025-06-29 13:31:03,412 - __main__ - INFO - [TCS.NS] Latest Strategy Signal (2025-06-27): HOLD at 3441.10\n",
            "2025-06-29 13:31:03,440 - analytics.ml_predictor - INFO - Next day movement prediction: Up\n",
            "2025-06-29 13:31:03,445 - __main__ - INFO - [TCS.NS] Next Day ML Prediction: Up\n",
            "2025-06-29 13:31:03,445 - sheets.google_sheets_manager - WARNING - Google Sheets connection not established. Cannot fetch alert configurations.\n",
            "2025-06-29 13:31:04,107 - utils.alerts - INFO - Telegram message sent successfully.\n",
            "2025-06-29 13:31:04,140 - strategy.trading_strategy - INFO - Signals generated successfully.\n",
            "2025-06-29 13:31:04,160 - __main__ - INFO - [HDFCBANK.NS] Latest Strategy Signal (2025-06-27): HOLD at 2014.90\n",
            "2025-06-29 13:31:04,180 - analytics.ml_predictor - INFO - Next day movement prediction: Down/No Change\n",
            "2025-06-29 13:31:04,180 - __main__ - INFO - [HDFCBANK.NS] Next Day ML Prediction: Down/No Change\n",
            "2025-06-29 13:31:04,180 - sheets.google_sheets_manager - WARNING - Google Sheets connection not established. Cannot fetch alert configurations.\n",
            "2025-06-29 13:31:04,825 - utils.alerts - INFO - Telegram message sent successfully.\n",
            "2025-06-29 13:31:04,826 - __main__ - INFO - Algo-Trading Prototype finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gspread\n",
        "print(f\"gspread version: {gspread.__version__}\")\n",
        "try:\n",
        "    gc = gspread.service_account(filename='/content/dummy_credentials.json') # Use a dummy path\n",
        "    print(f\"Type of gc: {type(gc)}\")\n",
        "    # This line will still error because dummy_credentials.json doesn't exist,\n",
        "    # but we're checking if gc is a gspread.Client before this point.\n",
        "    # If the error is still 'Client' object has no attribute 'open_by_id',\n",
        "    # and type(gc) is gspread.client.Client, then it's very bizarre.\n",
        "    # If type(gc) is something else, that's the root cause.\n",
        "    dummy_spreadsheet = gc.open_by_id(\"some_dummy_id\") # This will fail, but tests the method existence\n",
        "except Exception as e:\n",
        "    print(f\"Error in test: {e}\")"
      ],
      "metadata": {
        "id": "QOKU2ht6vX-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f6f0a8d-1689-4859-9d10-57dda4714365"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gspread version: 6.2.1\n",
            "Error in test: [Errno 2] No such file or directory: '/content/dummy_credentials.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w-BICD5C5mqP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}